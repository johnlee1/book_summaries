# Designing Data-Intensive Applications by Martin Kleppmann

## Chapter 1 - Reliable, Scalable, and Maintainable Applications
* "*Reliability* means making systems work correctly, even when faults occur. Faults can be in hardware (typically random and uncorrelated), 
software (bugs are typically systematic and hard to deal with), and humans (who inevitably make mistakes from time to time). Fault-tolerance techniques can hide 
certain types of faults from the end user." (22)
* "*Scalability* means having strategies for keeping performance good, even when load increases. In order to discuss scalability, we first need ways of describing 
load and performance quantitatively... In a scalable system, you can add processing capacity in order to remain reliable under high load." (22-23)
* "*Maintainability* has many facets, but in essence it's about making life better for the engineering and operations teams who need to work wih the system.
Good abstractions can help reduce complexity and make the system easier to modify and adapt for new use cases. Good operability means having good visibility into
the system's health, and having effective ways of managing it." (23)

## Chapter 2 - Data Models and Query Languages
* "The main arguments in favor of the document data model are schema flexibility, better performance due to locality, and that for some applications it is closer to the data structures used by the application. The relational model counters by providing better support for joins, and many-to-one and many-to-many relationships." (38)
* "*Document databases* target use cases where data comes in self-contained documents and relationships between one document and another are rare. *Graph databases* go in the opposite direction, targeting use cases where anything is potentially related to everything." (63)
* "One thing that document and graph databases have in common is that they typically don't enforce a schema for the data they store, which can make it easier to adapt applications to changing requirements. However, your application most likely still assumes that data has a certain structure; it's just a question of whether the schema is explicit (enforced on write) or implicit (handled on read)." (63)

## Chapter 3 - Storage and Retrieval
* "...storage engines fall into two broad categories: those optimized for transaction processing (OLTP), and those optimized for analytics (OLAP)." (103)
* "OLTP systems are typically user-facing, which means that they may see a huge volume of requests. In order to handle the load, applications usually only touch a small number of records in each query. The application requests records using some kind of key, and the storage engine uses an index to find the data for the requested key. Disk seek time is often the bottleneck here." (103)
* "Data warehouses and similar analytic systems are less well known, because they are primarily used by business analysts, not by end users. They handle a much lower volume of queries than OLTP systems, but each query is typically very demanding, requiring many millions of records to be scanned in a short time. Disk bandwidth (not seek time) is often the bottleneck here, and column-oriented storage is an increasingly popular solution for this kind of workload." (103)
* "On the OLTP side, we saw storage engines from two main schools of thought: The log-structured school, which only permits appending to files and deleting obsolete files, but never updates a file that has been written. The update-in-place school, which treats the disk as a set of fixed-size pages that can be overwritten. B-trees are the biggest example of this philosophy, being used in all major relational databases and also many nonrelational ones." (103)
* "Log-structured storage engines are a comparatively recent development. Their key idea is that they systematically turn random-access writes into sequential writes on disk, which enables higher write throughput due to the performance characteristics of hard drives and SSDs." (103)

## Chapter 4 - Encoding and Evolution
* "Programs usually work with data in (at least) two different representations: 1. In memory, data is kept in objects, structs, lists, arrays, hash tables, trees, and so on. These data structures are optimized for efficient access and manipulation by the CPU. 2. When you want to write data to a file or send it over the network, you have to encode it as some kind of self-contained sequence of bytes (for example, a JSON document). Thus, we need some kind of translation between the two representations. The translation from the in-memory representation to a byte sequence is called encoding (also known as serialization or marshalling), and the reverse is called decoding (parsing, deserialization, unmarshalling)." (112-113)
* "Many services need to support rolling upgrades, where a new version of a service is gradually deployed to a few nodes at a time, rather than deploying to all nodes simultaneously. Rolling upgrades allow new versions of a service ot be released without downtime (thus encouraging frequent small releases over rare big releases) and make deployments less risky (allowing faulty releases to be detected and rolled back before they affect a large number of users). These properties are hugely beneficial for evolvability, the ease of making changes to an application." (139)
* "During rolling upgrades, or for variuos other reasons, we must assume that different nodes are running the different versions of our application's code. Thus, it is important that all data flowing around the system is encoded in a way that provides backward compatibility (new code can read old data) and forward compatibility (old code can read new data)." (139)
